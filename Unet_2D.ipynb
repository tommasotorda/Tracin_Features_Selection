{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pip install numpy==1.18.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dltk.io.augmentation import *\n",
    "from dltk.io.preprocessing import *\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 07:25:12.857915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30967 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_train = glob(os.path.join(\"/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Data/DataTracin_train\", \"*\"))\n",
    "file_list_test = glob(os.path.join(\"/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Data/DataTracin_test\", \"*\"))\n",
    "\n",
    "X = np.array(np.load(file_list_train[0])['X_train'])\n",
    "Y = np.array(np.load(file_list_train[0])['Y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = X.shape[1]\n",
    "IMG_WIDTH  = X.shape[2]\n",
    "IMG_CHANNELS = X.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    data = np.load(image_file)\n",
    "    return data['X_train'], data['Y_train']\n",
    "\n",
    "def load_image_test(image_file):\n",
    "    data = np.load(image_file)\n",
    "    return data['X_test'], data['Y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_image, real_image, height, width):\n",
    "    shape = tf.shape(input_image)\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    real_image = tf.image.resize(real_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 4])\n",
    "\n",
    "    return cropped_image[0], cropped_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_translation(X,Y):\n",
    "    i = tf.random.uniform(shape = [], minval=-15, maxval=15, dtype=tf.dtypes.int32)\n",
    "    j = tf.random.uniform(shape = [], minval=-15, maxval=15, dtype=tf.dtypes.int32)\n",
    "    Y = tf.roll(Y, [i,j], axis = [0,1])\n",
    "    X = tf.roll(X, [i,j], axis = [0,1])\n",
    "    return X,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "        #A.HorizontalFlip(p=0.5),\n",
    "        #A.RandomRotate90(p=0.5),\n",
    "        #A.VerticalFlip(p=0.5),\n",
    "        #A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.6, p = 0.2),\n",
    "        A.ElasticTransform(p=0.2, alpha=200, sigma=200 * 0.06, alpha_affine=200 * 0.02),\n",
    "        #A.GaussianBlur(p = 0.2, blur_limit=(3, 7), sigma_limit=0),\n",
    "        #A.GaussNoise (p=0.2, var_limit=(0.2, 0.8), mean=0., per_channel=True, always_apply=False),\n",
    "        #A.OpticalDistortion (p = 0.5, distort_limit=0.05, shift_limit=0.05, interpolation=1, border_mode=4),\n",
    "        #A.PixelDropout (dropout_prob=0.02, per_channel=False, drop_value=-1, p=0.2),\n",
    "        #A.ZoomBlur (max_factor=1.11, step_factor=(0.01, 0.06), always_apply=False, p=0.5)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def aug_fn(image, mask):\n",
    "    aug = transforms(image = image, mask = mask)\n",
    "    aug_img, aug_mask = aug[\"image\"], aug[\"mask\"]\n",
    "\n",
    "    return aug_img, aug_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(image, mask):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    aug_img = tf.numpy_function(func=aug_fn, inp=[image, mask], Tout=[tf.float32, tf.float32])\n",
    "    return aug_img[0], aug_img[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, real_image):\n",
    "    input_image = tf.cast(input_image, tf.double)\n",
    "    input_image = tf.reshape(input_image, [192,192,4])\n",
    "    real_image = tf.reshape(real_image, [192,192,4])\n",
    "    \n",
    "    #if tf.random.uniform(())> 0.5:\n",
    "    #    input_image, real_image = random_translation(input_image, real_image)\n",
    "        \n",
    "    input_image, real_image = resize(input_image, real_image, 250, 250)\n",
    "\n",
    "    input_image, real_image = random_crop(input_image, real_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        real_image = tf.image.flip_left_right(real_image)\n",
    "        \n",
    "    #input_image, real_image = process_data(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image\n",
    "\n",
    "def test_reshape(X,Y):\n",
    "    X = tf.cast(X, tf.double)\n",
    "    X = tf.reshape(X, [192,192,4])\n",
    "    Y = tf.reshape(Y, [192,192,4])\n",
    "    return X,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 07:25:20.110474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30967 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.list_files('/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Data/DataTracin_train3/*.npz')\n",
    "train_dataset = train_dataset.map(lambda item: tf.numpy_function(\n",
    "          load_image_train, [item], [tf.double, tf.double]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.map(random_jitter, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "#train_dataset = train_dataset.map(process_data, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files('/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Data/DataTracin_test/*.npz')\n",
    "test_dataset = test_dataset.map(lambda item: tf.numpy_function(\n",
    "          load_image_test, [item], [tf.double, tf.double]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.map(test_reshape)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)   #Not in the original network. \n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)  #Not in the original network\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = tf.keras.layers.MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = tf.keras.layers.Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 192, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "unet = build_unet(input_shape,Y.shape[-1])\n",
    "tf.keras.utils.plot_model(unet, show_shapes=True, dpi=64);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "  prediction = model(test_input, training=True)\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  display_list = [test_input[0,:,:,1], np.argmax(tar[0], axis =2), np.argmax(prediction[0], axis = 2)]\n",
    "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "  for i in range(3):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.title(title[i])\n",
    "    # Getting the pixel values in the [0, 1] range to plot.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet_optimizer = tf.keras.optimizers.SGD(learning_rate=2e-4, momentum=0.0)\n",
    "unet_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5, epsilon = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice0(y_true, y_pred, smooth = 1e-7):\n",
    "    y_true_f = tf.reshape(tf.cast(y_true[:,:,:,0], 'float32'), [-1]) \n",
    "    y_pred_f = tf.reshape(tf.cast(y_pred[:,:,:,0], 'float32'), [-1])\n",
    "    return (2*tf.reduce_sum(tf.abs(y_true_f*y_pred_f)))/(tf.reduce_sum(\n",
    "        y_true_f**2 + y_pred_f**2)+smooth)\n",
    "\n",
    "def dice1(y_true, y_pred, smooth = 1e-7):  \n",
    "    y_true_f = tf.reshape(tf.cast(y_true[:,:,:,1], 'float32'), [-1]) \n",
    "    y_pred_f = tf.reshape(tf.cast(y_pred[:,:,:,1], 'float32'), [-1])\n",
    "    return (2*tf.reduce_sum(tf.abs(y_true_f*y_pred_f)))/(tf.reduce_sum(\n",
    "        y_true_f**2 + y_pred_f**2)+smooth)\n",
    "\n",
    "def dice2(y_true, y_pred, smooth = 1e-7):\n",
    "    y_true_f = tf.reshape(tf.cast(y_true[:,:,:,2], 'float32'), [-1]) \n",
    "    y_pred_f = tf.reshape(tf.cast(y_pred[:,:,:,2], 'float32'), [-1])\n",
    "    return (2*tf.reduce_sum(tf.abs(y_true_f*y_pred_f)))/(tf.reduce_sum(\n",
    "        y_true_f**2 + y_pred_f**2)+smooth)\n",
    "\n",
    "def dice3(y_true, y_pred, smooth = 1e-7):  \n",
    "    y_true_f = tf.reshape(tf.cast(y_true[:,:,:,3], 'float32'), [-1]) \n",
    "    y_pred_f = tf.reshape(tf.cast(y_pred[:,:,:,3], 'float32'), [-1])\n",
    "    return (2*tf.reduce_sum(tf.abs(y_true_f*y_pred_f)))/(tf.reduce_sum(\n",
    "        y_true_f**2 + y_pred_f**2)+smooth)\n",
    "\n",
    "def cce(y_true, y_pred):\n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "def dice_loss1(y_true, y_pred):\n",
    "    a0 = 0\n",
    "    a1 = 1\n",
    "    a2 = 1\n",
    "    a3 = 1\n",
    "    return 1-(a0*dice0(y_true,y_pred)+a1*dice1(y_true,y_pred)+a2*dice2(\n",
    "        y_true,y_pred)+a3*dice3(y_true,y_pred))/(a0+a1+a2+a3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./Crossvalidation/Train3/model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "checkpoint_dir = './Crossvalidation/Train3/model2'\n",
    "checkpoint_filepath = os.path.join(checkpoint_dir, \"ckpt_Unet_{epoch:02d}\")\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose = 1,\n",
    "    save_weights_only=False,\n",
    "    monitor=\"val_loss\",\n",
    "    mode='min',\n",
    "    save_best_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(optimizer=unet_optimizer, loss=dice_loss1, metrics=[dice0,dice1,dice2,dice3, cce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 07:25:38.686311: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-09-15 07:25:42.437849: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 45s 171ms/step - loss: 0.5668 - dice0: 0.6076 - dice1: 0.0232 - dice2: 0.6890 - dice3: 0.5875 - cce: 2.0303 - val_loss: 0.7483 - val_dice0: 0.8280 - val_dice1: 0.0159 - val_dice2: 0.6169 - val_dice3: 0.1222 - val_cce: 0.8603\n",
      "\n",
      "Epoch 00001: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 07:26:25.034598: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_01/assets\n",
      "Epoch 2/30\n",
      "187/187 [==============================] - 28s 139ms/step - loss: 0.4405 - dice0: 0.6058 - dice1: 0.0249 - dice2: 0.8237 - dice3: 0.8298 - cce: 2.3717 - val_loss: 0.5014 - val_dice0: 0.8202 - val_dice1: 0.0210 - val_dice2: 0.7033 - val_dice3: 0.7717 - val_cce: 1.1215\n",
      "\n",
      "Epoch 00002: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_02\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_02/assets\n",
      "Epoch 3/30\n",
      "187/187 [==============================] - 29s 145ms/step - loss: 0.4288 - dice0: 0.6629 - dice1: 0.0297 - dice2: 0.8483 - dice3: 0.8355 - cce: 2.2409 - val_loss: 0.4898 - val_dice0: 0.8599 - val_dice1: 0.0293 - val_dice2: 0.8127 - val_dice3: 0.6887 - val_cce: 1.0998\n",
      "\n",
      "Epoch 00003: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_03\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_03/assets\n",
      "Epoch 4/30\n",
      "187/187 [==============================] - 29s 140ms/step - loss: 0.4012 - dice0: 0.8021 - dice1: 0.0638 - dice2: 0.8678 - dice3: 0.8647 - cce: 1.6616 - val_loss: 0.5019 - val_dice0: 0.9096 - val_dice1: 0.0367 - val_dice2: 0.7532 - val_dice3: 0.7046 - val_cce: 0.4474\n",
      "\n",
      "Epoch 00004: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_04\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_04/assets\n",
      "Epoch 5/30\n",
      "187/187 [==============================] - 27s 132ms/step - loss: 0.2113 - dice0: 0.9933 - dice1: 0.6822 - dice2: 0.8446 - dice3: 0.8391 - cce: 0.0913 - val_loss: 0.2631 - val_dice0: 0.9930 - val_dice1: 0.6325 - val_dice2: 0.7888 - val_dice3: 0.7895 - val_cce: 0.0862\n",
      "\n",
      "Epoch 00005: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_05\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_05/assets\n",
      "Epoch 6/30\n",
      "187/187 [==============================] - 28s 138ms/step - loss: 0.1655 - dice0: 0.9951 - dice1: 0.7802 - dice2: 0.8584 - dice3: 0.8649 - cce: 0.0615 - val_loss: 0.2385 - val_dice0: 0.9946 - val_dice1: 0.6852 - val_dice2: 0.8118 - val_dice3: 0.7875 - val_cce: 0.0624\n",
      "\n",
      "Epoch 00006: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_06\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_06/assets\n",
      "Epoch 7/30\n",
      "187/187 [==============================] - 28s 138ms/step - loss: 0.1496 - dice0: 0.9956 - dice1: 0.8002 - dice2: 0.8734 - dice3: 0.8775 - cce: 0.0529 - val_loss: 0.2674 - val_dice0: 0.9933 - val_dice1: 0.6712 - val_dice2: 0.7405 - val_dice3: 0.7860 - val_cce: 0.0748\n",
      "\n",
      "Epoch 00007: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_07\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_07/assets\n",
      "Epoch 8/30\n",
      "187/187 [==============================] - 29s 140ms/step - loss: 0.1354 - dice0: 0.9960 - dice1: 0.8243 - dice2: 0.8826 - dice3: 0.8869 - cce: 0.0471 - val_loss: 0.2428 - val_dice0: 0.9952 - val_dice1: 0.6597 - val_dice2: 0.8012 - val_dice3: 0.8107 - val_cce: 0.0581\n",
      "\n",
      "Epoch 00008: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_08\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_08/assets\n",
      "Epoch 9/30\n",
      "187/187 [==============================] - 27s 134ms/step - loss: 0.1362 - dice0: 0.9962 - dice1: 0.8163 - dice2: 0.8883 - dice3: 0.8868 - cce: 0.0440 - val_loss: 0.2564 - val_dice0: 0.9953 - val_dice1: 0.6473 - val_dice2: 0.8253 - val_dice3: 0.7582 - val_cce: 0.0574\n",
      "\n",
      "Epoch 00009: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_09\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_09/assets\n",
      "Epoch 10/30\n",
      "187/187 [==============================] - 28s 139ms/step - loss: 0.1263 - dice0: 0.9963 - dice1: 0.8334 - dice2: 0.8925 - dice3: 0.8951 - cce: 0.0417 - val_loss: 0.2353 - val_dice0: 0.9954 - val_dice1: 0.6448 - val_dice2: 0.8330 - val_dice3: 0.8163 - val_cce: 0.0559\n",
      "\n",
      "Epoch 00010: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_10\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_10/assets\n",
      "Epoch 11/30\n",
      "187/187 [==============================] - 28s 136ms/step - loss: 0.1290 - dice0: 0.9962 - dice1: 0.8332 - dice2: 0.8911 - dice3: 0.8885 - cce: 0.0416 - val_loss: 0.2152 - val_dice0: 0.9960 - val_dice1: 0.7210 - val_dice2: 0.8487 - val_dice3: 0.7845 - val_cce: 0.0451\n",
      "\n",
      "Epoch 00011: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_11\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_11/assets\n",
      "Epoch 12/30\n",
      "187/187 [==============================] - 28s 136ms/step - loss: 0.1438 - dice0: 0.9959 - dice1: 0.8073 - dice2: 0.8810 - dice3: 0.8803 - cce: 0.0445 - val_loss: 0.2448 - val_dice0: 0.9953 - val_dice1: 0.6504 - val_dice2: 0.8043 - val_dice3: 0.8109 - val_cce: 0.0579\n",
      "\n",
      "Epoch 00012: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_12\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_12/assets\n",
      "Epoch 13/30\n",
      "187/187 [==============================] - 29s 140ms/step - loss: 0.1380 - dice0: 0.9961 - dice1: 0.8118 - dice2: 0.8876 - dice3: 0.8866 - cce: 0.0424 - val_loss: 0.2490 - val_dice0: 0.9947 - val_dice1: 0.6653 - val_dice2: 0.8107 - val_dice3: 0.7771 - val_cce: 0.0574\n",
      "\n",
      "Epoch 00013: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_13\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_13/assets\n",
      "Epoch 14/30\n",
      "187/187 [==============================] - 29s 140ms/step - loss: 0.1135 - dice0: 0.9968 - dice1: 0.8513 - dice2: 0.9047 - dice3: 0.9033 - cce: 0.0354 - val_loss: 0.1937 - val_dice0: 0.9957 - val_dice1: 0.7398 - val_dice2: 0.8559 - val_dice3: 0.8233 - val_cce: 0.0463\n",
      "\n",
      "Epoch 00014: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_14\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_14/assets\n",
      "Epoch 15/30\n",
      "187/187 [==============================] - 29s 141ms/step - loss: 0.1070 - dice0: 0.9970 - dice1: 0.8604 - dice2: 0.9100 - dice3: 0.9087 - cce: 0.0330 - val_loss: 0.2404 - val_dice0: 0.9952 - val_dice1: 0.6796 - val_dice2: 0.8063 - val_dice3: 0.7929 - val_cce: 0.0549\n",
      "\n",
      "Epoch 00015: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_15\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_15/assets\n",
      "Epoch 16/30\n",
      "187/187 [==============================] - 41s 208ms/step - loss: 0.1148 - dice0: 0.9968 - dice1: 0.8515 - dice2: 0.9024 - dice3: 0.9018 - cce: 0.0347 - val_loss: 0.1907 - val_dice0: 0.9960 - val_dice1: 0.7392 - val_dice2: 0.8669 - val_dice3: 0.8220 - val_cce: 0.0432\n",
      "\n",
      "Epoch 00016: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_16\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_16/assets\n",
      "Epoch 17/30\n",
      "187/187 [==============================] - 41s 209ms/step - loss: 0.1062 - dice0: 0.9970 - dice1: 0.8614 - dice2: 0.9088 - dice3: 0.9113 - cce: 0.0324 - val_loss: 0.2746 - val_dice0: 0.9950 - val_dice1: 0.6290 - val_dice2: 0.7378 - val_dice3: 0.8094 - val_cce: 0.0616\n",
      "\n",
      "Epoch 00017: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_17\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_17/assets\n",
      "Epoch 18/30\n",
      "187/187 [==============================] - 41s 205ms/step - loss: 0.1008 - dice0: 0.9971 - dice1: 0.8696 - dice2: 0.9135 - dice3: 0.9146 - cce: 0.0313 - val_loss: 0.2274 - val_dice0: 0.9954 - val_dice1: 0.7009 - val_dice2: 0.8321 - val_dice3: 0.7848 - val_cce: 0.0495\n",
      "\n",
      "Epoch 00018: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_18\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_18/assets\n",
      "Epoch 19/30\n",
      "187/187 [==============================] - 42s 210ms/step - loss: 0.0942 - dice0: 0.9973 - dice1: 0.8800 - dice2: 0.9184 - dice3: 0.9191 - cce: 0.0291 - val_loss: 0.2150 - val_dice0: 0.9961 - val_dice1: 0.6995 - val_dice2: 0.8509 - val_dice3: 0.8046 - val_cce: 0.0443\n",
      "\n",
      "Epoch 00019: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_19\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_19/assets\n",
      "Epoch 20/30\n",
      "187/187 [==============================] - 42s 211ms/step - loss: 0.0975 - dice0: 0.9972 - dice1: 0.8735 - dice2: 0.9160 - dice3: 0.9180 - cce: 0.0298 - val_loss: 0.2325 - val_dice0: 0.9953 - val_dice1: 0.6834 - val_dice2: 0.8374 - val_dice3: 0.7817 - val_cce: 0.0533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_20\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_20/assets\n",
      "Epoch 21/30\n",
      "187/187 [==============================] - 42s 212ms/step - loss: 0.1016 - dice0: 0.9971 - dice1: 0.8687 - dice2: 0.9115 - dice3: 0.9149 - cce: 0.0308 - val_loss: 0.2346 - val_dice0: 0.9959 - val_dice1: 0.6793 - val_dice2: 0.8245 - val_dice3: 0.7923 - val_cce: 0.0482\n",
      "\n",
      "Epoch 00021: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_21\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_21/assets\n",
      "Epoch 22/30\n",
      "187/187 [==============================] - 41s 209ms/step - loss: 0.0960 - dice0: 0.9972 - dice1: 0.8731 - dice2: 0.9173 - dice3: 0.9215 - cce: 0.0290 - val_loss: 0.2366 - val_dice0: 0.9952 - val_dice1: 0.6866 - val_dice2: 0.8167 - val_dice3: 0.7868 - val_cce: 0.0550\n",
      "\n",
      "Epoch 00022: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_22\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_22/assets\n",
      "Epoch 23/30\n",
      "187/187 [==============================] - 40s 203ms/step - loss: 0.1063 - dice0: 0.9970 - dice1: 0.8573 - dice2: 0.9119 - dice3: 0.9120 - cce: 0.0312 - val_loss: 0.2232 - val_dice0: 0.9949 - val_dice1: 0.6989 - val_dice2: 0.8287 - val_dice3: 0.8027 - val_cce: 0.0576\n",
      "\n",
      "Epoch 00023: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_23\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_23/assets\n",
      "Epoch 24/30\n",
      "187/187 [==============================] - 42s 212ms/step - loss: 0.0997 - dice0: 0.9973 - dice1: 0.8641 - dice2: 0.9179 - dice3: 0.9189 - cce: 0.0290 - val_loss: 0.2658 - val_dice0: 0.9956 - val_dice1: 0.6049 - val_dice2: 0.7986 - val_dice3: 0.7989 - val_cce: 0.0567\n",
      "\n",
      "Epoch 00024: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_24\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_24/assets\n",
      "Epoch 25/30\n",
      "187/187 [==============================] - 41s 205ms/step - loss: 0.0983 - dice0: 0.9973 - dice1: 0.8711 - dice2: 0.9162 - dice3: 0.9177 - cce: 0.0290 - val_loss: 0.2211 - val_dice0: 0.9955 - val_dice1: 0.7108 - val_dice2: 0.8399 - val_dice3: 0.7859 - val_cce: 0.0491\n",
      "\n",
      "Epoch 00025: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_25\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_25/assets\n",
      "Epoch 26/30\n",
      "187/187 [==============================] - 41s 206ms/step - loss: 0.0857 - dice0: 0.9975 - dice1: 0.8948 - dice2: 0.9269 - dice3: 0.9212 - cce: 0.0261 - val_loss: 0.2204 - val_dice0: 0.9958 - val_dice1: 0.7095 - val_dice2: 0.8356 - val_dice3: 0.7938 - val_cce: 0.0488\n",
      "\n",
      "Epoch 00026: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_26\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_26/assets\n",
      "Epoch 27/30\n",
      "187/187 [==============================] - 41s 205ms/step - loss: 0.0876 - dice0: 0.9975 - dice1: 0.8868 - dice2: 0.9256 - dice3: 0.9249 - cce: 0.0259 - val_loss: 0.2009 - val_dice0: 0.9960 - val_dice1: 0.7237 - val_dice2: 0.8542 - val_dice3: 0.8194 - val_cce: 0.0442\n",
      "\n",
      "Epoch 00027: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_27\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_27/assets\n",
      "Epoch 28/30\n",
      "187/187 [==============================] - 42s 210ms/step - loss: 0.0989 - dice0: 0.9974 - dice1: 0.8667 - dice2: 0.9221 - dice3: 0.9146 - cce: 0.0276 - val_loss: 0.2042 - val_dice0: 0.9957 - val_dice1: 0.7159 - val_dice2: 0.8566 - val_dice3: 0.8149 - val_cce: 0.0456\n",
      "\n",
      "Epoch 00028: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_28\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_28/assets\n",
      "Epoch 29/30\n",
      "187/187 [==============================] - 41s 209ms/step - loss: 0.0897 - dice0: 0.9976 - dice1: 0.8816 - dice2: 0.9274 - dice3: 0.9221 - cce: 0.0255 - val_loss: 0.2046 - val_dice0: 0.9963 - val_dice1: 0.7225 - val_dice2: 0.8621 - val_dice3: 0.8016 - val_cce: 0.0424\n",
      "\n",
      "Epoch 00029: saving model to ./Crossvalidation/Train3/model2/ckpt_Unet_29\n",
      "INFO:tensorflow:Assets written to: ./Crossvalidation/Train3/model2/ckpt_Unet_29/assets\n",
      "Epoch 30/30\n",
      "187/187 [==============================] - 30s 148ms/step - loss: 0.0962 - dice0: 0.9973 - dice1: 0.8761 - dice2: 0.9165 - dice3: 0.9188 - cce: 0.0279 - val_loss: 0.2415 - val_dice0: 0.9953 - val_dice1: 0.6764 - val_dice2: 0.8166 - val_dice3: 0.7825 - val_cce: 0.0522\n"
     ]
    }
   ],
   "source": [
    "history = unet.fit(train_dataset,\n",
    "                    verbose=1, \n",
    "                    epochs=epochs,\n",
    "                    #steps_per_epoch =X_train.shape[0]//BATCH_SIZE-1,\n",
    "                    #validation_steps = X.shape[0]//BATCH_SIZE-1,\n",
    "                    validation_data=test_dataset, \n",
    "                    callbacks=[model_checkpoint_callback],\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.save('./Modelli/Unet_Allexp_60_120slice_50epch_Alldice_DataCgan_09_02_2022.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Train/History6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Train/History6/unet_history.json', 'w') as handle: # saving the history of the model\n",
    "    json.dump(history.history, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/tordatom/Dati_Imaging/BraTs_19/Segmentation2D/Crossvalidation/Train_nodef/History2/unet_history.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = json.load(open(path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "xposition = [27,28,29]\n",
    "for xc in xposition:\n",
    "    plt.axvline(x=xc, color='orange', linestyle='--')\n",
    "    if xc == len(xposition): plt.axvline(x=xc, color='orange', linestyle='--', label = \"ckpt\")\n",
    "    else: plt.axvline(x=xc, color='orange', linestyle='--')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlim([0.8,30.5])\n",
    "plt.ylim([0,0.5])\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Images/Unet_history_finalckpt\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlim([0,30.5])\n",
    "plt.ylim([0,1.4])\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "rec = history['dice0']\n",
    "val_rec = history['val_dice0']\n",
    "\n",
    "plt.plot(epochs, rec, 'y', label='Training Dice 0 ')\n",
    "plt.plot(epochs, val_rec, 'r', label='Validation Dice 0')\n",
    "plt.title('Training and Validation Dice 0')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice0')\n",
    "plt.legend()\n",
    "#plt.savefig(\"Images/Unet_Dice0\")\n",
    "plt.show()\n",
    "\n",
    "rec1 = history['dice1']\n",
    "val_rec1 = history['val_dice1']\n",
    "\n",
    "plt.plot(epochs, rec1, 'y', label='Training Dice 1 ')\n",
    "plt.plot(epochs, val_rec1, 'r', label='Validation Dice 1')\n",
    "plt.title('Training and Validation Dice 1')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice1')\n",
    "plt.xlim([0,30.5])\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend()\n",
    "#plt.savefig(\"Images/Unet_Dice1\")\n",
    "plt.show()\n",
    "\n",
    "rec2 = history['dice2']\n",
    "val_rec2 = history['val_dice2']\n",
    "\n",
    "\n",
    "plt.plot(epochs, rec2, 'y', label='Training Dice 2')\n",
    "plt.plot(epochs, val_rec2, 'r', label='Validation Dice 2')\n",
    "\n",
    "plt.title('Training and Validation Dice 2')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice2')\n",
    "plt.xlim([0,30.5])\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend()\n",
    "#plt.savefig(\"Images/Unet_Dice2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "rec3 = history['dice3']\n",
    "val_rec3 = history['val_dice3']\n",
    "\n",
    "l = [10,18,26,31]\n",
    "plt.plot(epochs, rec3, 'y', label='Training Dice 3')\n",
    "plt.plot(epochs, val_rec3, 'r', label='Validation Dice 3')\n",
    "for i in l:\n",
    "    plt.axvline(x=i, color='red', linestyle='--')\n",
    "\n",
    "plt.title('Training and Validation Dice 3')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice3')\n",
    "plt.xlim([0,30.5])\n",
    "plt.ylim([0,1.0])\n",
    "plt.legend()\n",
    "#plt.savefig(\"Images/Unet_Dice3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model(\"Modelli/Unet_Allexp_60_120slice_50epch_Alldice_DataCgan_09_02_2022.hdf5\",\n",
    "                                custom_objects={'dice0': dice0, 'dice1': dice1, 'dice2': dice2, 'dice3': dice3, \"dice_loss1\":dice_loss1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in test_dataset:\n",
    "    y = np.argmax(unet.predict(i), axis = 3)\n",
    "    a = np.argmax(j, axis = 3)\n",
    "    for k in range(y.shape[0]):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(231)\n",
    "        plt.title(\"Channel 0\")\n",
    "        plt.imshow(i[k,:,:,0])\n",
    "        plt.subplot(232)\n",
    "        plt.title(\"Predicted Seg\")\n",
    "        plt.imshow(y[k])\n",
    "        plt.subplot(233)\n",
    "        plt.title(\"Ground Truth \")\n",
    "        plt.imshow(a[k])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in test_dataset:\n",
    "    y = np.argmax(unet.predict(i), axis = 3)\n",
    "    a = np.argmax(j, axis = 3)\n",
    "    a = np.ma.masked_where(a == 0, a)\n",
    "\n",
    "    for k in range(y.shape[0]):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.title(\"BraTs segmentation\")\n",
    "        plt.imshow(i[k,:,:,0], cmap = \"gist_gray\")\n",
    "        #plt.title(\"Predicted Seg\")\n",
    "        #plt.imshow(y[k], alpha = 0.1)\n",
    "        #plt.title(\"Ground Truth \")\n",
    "        plt.imshow(a[k], alpha = 0.5)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
